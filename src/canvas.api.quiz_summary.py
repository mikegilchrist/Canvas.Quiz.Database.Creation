#!/usr/bin/env python3
# ============================================================================
# Script:       canvas.api.quiz_summary.py
# Version:      1.0.0
# Date:         2026-02-20
# Purpose:      Generate a per-student quiz summary report with time spent,
#               word count, words-per-minute, and score breakdown.
#
# Usage:        canvas.api.quiz_summary.py COURSE_ID [QUIZ_ID]
#                   [--base-url URL] [--token TOKEN | --token-file FILE]
#                   [--outdir DIR] [--anonymize] [--update]
#                   [--include-surveys] [--dry-run] [-v | -vv] [--help]
#
# Input:        Canvas API token + course_id (or ~/.canvas.api.conf)
# Output:       Per-student summary table to stdout; optional CSV to --outdir
#
# Verbosity:
#   (default)   Student, score, time, word count, wpm, correct/total
#   -v          Adds per-question breakdown (type, correct, points)
#   -vv         Adds full essay response text
#
# Options:
#   --base-url URL          Canvas instance URL (or set in ~/.canvas.api.conf)
#   --token TOKEN           Inline API token
#   --token-file FILE       Path to token file (or set in ~/.canvas.api.conf)
#   --outdir DIR            Output directory for CSV (default: stdout only)
#   --anonymize             Replace student names with Student_01..NN
#   -u, --update            Skip quizzes whose CSV already exists
#   --include-surveys       Include survey-type quizzes (batch mode)
#   -n, --dry-run           Show what would be done without making changes
#   -v, --verbose           Increase verbosity (repeatable: -v, -vv)
#   -h, --help              Show this help message
#
# Session:      Canvas.API
# AI Model:     Claude Opus 4.6
# Attribution:  Generated by Michael Gilchrist in collaboration with
#               ClaudeAI Opus 4.6
# ============================================================================

import argparse
import csv
import os
import re
import sys

from canvas_api import (
    get_quiz,
    get_quiz_submission_data,
    get_quiz_submissions,
    get_quizzes,
    get_submission_questions,
)
from canvas_http import http_get_json
from io_utils import load_profile, read_token, sanitize_for_filename
from model import build_quiz_question_lookup


# ---- Helpers ----

def strip_html(text):
    """Remove HTML tags and collapse whitespace."""
    if not text:
        return ""
    clean = re.sub(r"<[^>]+>", " ", str(text))
    clean = re.sub(r"&nbsp;", " ", clean)
    clean = re.sub(r"&amp;", "&", clean)
    clean = re.sub(r"&lt;", "<", clean)
    clean = re.sub(r"&gt;", ">", clean)
    clean = re.sub(r"&#\d+;", " ", clean)
    clean = re.sub(r"\s+", " ", clean).strip()
    return clean


def word_count(text):
    """Count words in plain text."""
    text = strip_html(text)
    if not text:
        return 0
    return len(text.split())


ESSAY_TYPES = {"essay_question", "short_answer_question"}



def _get_quiz_question_metadata(base_url, token, quiz_submission_id, verbose):
    """Fetch quiz question metadata via the Quiz Submission Questions API."""
    url = (f"{base_url}/api/v1/quiz_submissions/{quiz_submission_id}"
           f"/questions?include[]=quiz_question&per_page=100")
    if verbose >= 1:
        print(f"  [GET] {url}")
    try:
        data, _headers = http_get_json(url, token)
        return data.get("quiz_questions", [])
    except Exception as e:
        if verbose >= 1:
            print(f"  [WARN] Could not fetch quiz question metadata: {e}")
        return []


# ---- Per-quiz summary logic ----

def summarize_one_quiz(base_url, token, course_id, quiz, verbose=0):
    """Build per-student summary data for one quiz.

    Returns (quiz_title, points_possible, questions_total,
             qq_lookup, student_rows) where student_rows is a list of dicts.
    """
    qid = quiz["id"]
    title = quiz.get("title", "(untitled)")
    points_possible = quiz.get("points_possible", 0)

    # Fetch quiz submissions (timing, score, user_id)
    quiz_subs = get_quiz_submissions(
        base_url, token, course_id, qid, verbose=(verbose >= 1))

    if not quiz_subs:
        return title, points_possible, 0, {}, []

    # Fetch assignment submissions with submission_history
    assignment_id = quiz.get("assignment_id")
    if not assignment_id:
        print(f"  [WARN] Quiz {qid} has no linked assignment", file=sys.stderr)
        return title, points_possible, 0, {}, []

    assign_subs = get_quiz_submission_data(
        base_url, token, course_id, assignment_id, verbose=(verbose >= 1))

    # Build user_id -> (submission_data, user_name)
    user_sub_data = {}
    user_names = {}
    for asub in assign_subs:
        uid = asub.get("user_id")
        user_obj = asub.get("user")
        if user_obj and isinstance(user_obj, dict):
            user_names[uid] = user_obj.get("sortable_name") or \
                              user_obj.get("name") or str(uid)
        history = asub.get("submission_history", [])
        if history:
            latest = history[-1]
            sd = latest.get("submission_data")
            if sd:
                user_sub_data[uid] = sd

    # Fetch quiz question metadata from first submission
    qq_lookup = {}
    if quiz_subs:
        first_sid = quiz_subs[0].get("id")
        if first_sid:
            qq_list = _get_quiz_question_metadata(
                base_url, token, first_sid, verbose=verbose)
            qq_lookup = build_quiz_question_lookup(qq_list)

    questions_total = len(qq_lookup) if qq_lookup else 0

    # Build per-student rows
    rows = []
    for qs in quiz_subs:
        uid = qs.get("user_id")
        sid = qs.get("id")
        time_spent = qs.get("time_spent")  # seconds
        score = qs.get("score")

        sd = user_sub_data.get(uid, [])

        # Calculate metrics
        total_words = 0
        question_details = []

        for item in sd:
            q_id = item.get("question_id")
            meta = qq_lookup.get(q_id, {}) if q_id else {}
            q_type = meta.get("question_type", "unknown")
            raw_correct = item.get("correct")
            pts = item.get("points", 0) or 0
            pts_possible = meta.get("points_possible", 0) or 0

            # Word count for essay-type questions
            q_words = 0
            response_text = ""
            if q_type in ESSAY_TYPES:
                raw = item.get("text", "")
                response_text = raw
                q_words = word_count(raw)
                total_words += q_words

            question_details.append({
                "question_id": q_id,
                "position": meta.get("position"),
                "question_type": q_type,
                "question_name": meta.get("question_name", ""),
                "correct": raw_correct,
                "points": pts,
                "points_possible": pts_possible,
                "word_count": q_words,
                "response_text": response_text,
                "prompt": meta.get("question_text", ""),
            })

        time_sec = time_spent or 0
        time_hr = round(time_sec / 3600.0, 1)
        time_min = time_sec / 60.0
        wpm = round(total_words / time_min, 1) if time_min > 0 else 0
        pct = round(100.0 * score / points_possible, 1) \
            if (score is not None and points_possible) else None

        rows.append({
            "submission_id": sid,
            "student_id": uid,
            "student_name": user_names.get(uid, str(uid)),
            "score": score,
            "pct": pct,
            "time_hr": time_hr,
            "word_count": total_words,
            "wpm": wpm,
            "questions": question_details,
        })

    # Sort by student name
    rows.sort(key=lambda r: r.get("student_name", ""))

    return title, points_possible, questions_total, qq_lookup, rows


# ---- Output formatting ----

def print_summary(title, points_possible, questions_total, rows,
                  anonymize, verbose=0):
    """Print summary table to stdout."""
    print(f"\n{'=' * 70}")
    print(f"Quiz: {title}")
    print(f"Points possible: {points_possible}  |  "
          f"Questions: {questions_total}  |  "
          f"Submissions: {len(rows)}")
    print(f"{'=' * 70}")

    if not rows:
        print("  (no submissions)")
        return

    # Anonymize names
    if anonymize:
        for i, r in enumerate(rows, 1):
            r["student_name"] = f"Student_{i:02d}"

    # Header
    print(f"{'Student':<25} {'Score':>6} {'Pct':>6} {'Hours':>7} "
          f"{'Words':>6} {'WPM':>6}")
    print(f"{'-' * 25} {'-' * 6} {'-' * 6} {'-' * 7} "
          f"{'-' * 6} {'-' * 6}")

    for r in rows:
        score_str = f"{r['score']}" if r['score'] is not None else "-"
        pct_str = f"{r['pct']:.0f}%" if r['pct'] is not None else "-"
        name = r["student_name"][:25]

        print(f"{name:<25} {score_str:>6} {pct_str:>6} {r['time_hr']:>7.1f} "
              f"{r['word_count']:>6} {r['wpm']:>6.1f}")

        # -v: per-question breakdown
        if verbose >= 1:
            for qd in r["questions"]:
                pos = qd.get("position", "?")
                qtype = qd.get("question_type", "?")
                pts = qd.get("points", 0)
                pts_p = qd.get("points_possible", 0)
                raw_correct = qd.get("correct")
                # MC/TF: show Y/N; essay: show points fraction
                if raw_correct is True:
                    status = "Y"
                elif raw_correct is False:
                    status = "N"
                else:
                    status = f"{pts}/{pts_p}"
                wc = qd.get("word_count", 0)
                wc_str = f"  ({wc}w)" if wc > 0 else ""
                print(f"    Q{pos}: {qtype:<25} "
                      f"{status:>8} pts{wc_str}")

                # -vv: full response text for essay questions
                if verbose >= 2 and qd.get("response_text"):
                    text = strip_html(qd["response_text"])
                    # Wrap at 70 chars with indent
                    lines = []
                    while text:
                        lines.append(text[:66])
                        text = text[66:]
                    for line in lines:
                        print(f"      | {line}")

    # Summary stats
    times = [r["time_hr"] for r in rows if r["time_hr"] > 0]
    wpms = [r["wpm"] for r in rows if r["wpm"] > 0]
    pcts = [r["pct"] for r in rows if r["pct"] is not None]
    if pcts:
        print(f"\nScore: min={min(pcts):.0f}%  "
              f"max={max(pcts):.0f}%  "
              f"median={sorted(pcts)[len(pcts)//2]:.0f}%")
    if times:
        print(f"Hours: min={min(times):.1f}  "
              f"max={max(times):.1f}  "
              f"median={sorted(times)[len(times)//2]:.1f}")
    if wpms:
        print(f"WPM:   min={min(wpms):.1f}  "
              f"max={max(wpms):.1f}  "
              f"median={sorted(wpms)[len(wpms)//2]:.1f}")


def write_summary_csv(filepath, title, points_possible, questions_total,
                      rows, anonymize):
    """Write summary CSV file."""
    if anonymize:
        for i, r in enumerate(rows, 1):
            r["student_name"] = f"Student_{i:02d}"

    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([
            "student_name", "score", "points_possible", "pct",
            "time_hr", "word_count", "wpm",
        ])
        for r in rows:
            writer.writerow([
                r["student_name"],
                r["score"],
                points_possible,
                r["pct"],
                r["time_hr"],
                r["word_count"],
                r["wpm"],
            ])


# ---- CLI ----

def main():
    ap = argparse.ArgumentParser(
        description="Generate per-student quiz summary with time spent, "
                    "word count, and words-per-minute.  Pass a QUIZ_ID "
                    "for a single quiz, or omit it for all quizzes.")
    ap.add_argument("course_id", type=int,
                    help="Canvas course ID")
    ap.add_argument("quiz_id", nargs="?", type=int, default=None,
                    help="Canvas quiz ID (omit for all quizzes)")

    ap.add_argument("--base-url", default=None,
                    help="Canvas instance URL (or set base_url in "
                         "~/.canvas.api.conf)")
    ap.add_argument("--token", default=None,
                    help="Canvas API token (inline)")
    ap.add_argument("--token-file", default=None,
                    help="Path to token file (or set token_file in "
                         "~/.canvas.api.conf)")

    ap.add_argument("--outdir", default=None,
                    help="Output directory for CSV (omit for stdout only)")
    ap.add_argument("--anonymize", action="store_true",
                    help="Replace student names with Student_01..NN")
    ap.add_argument("-u", "--update", action="store_true",
                    help="Skip quizzes whose summary CSV already exists")
    ap.add_argument("--include-surveys", action="store_true",
                    help="Include survey-type quizzes (batch mode)")

    ap.add_argument("-n", "--dry-run", action="store_true",
                    help="Show what would be done without making changes")
    ap.add_argument("-v", "--verbose", action="count", default=0,
                    help="Increase verbosity (-v per-question, "
                         "-vv full response text)")

    args = ap.parse_args()

    profile = load_profile()
    if not args.base_url:
        args.base_url = profile.get("base_url")
    if not args.base_url:
        ap.error("--base-url is required (or set base_url in "
                 "~/.canvas.api.conf)")
    token = read_token(args.token, args.token_file, profile)
    base_url = args.base_url.rstrip("/")

    # Build quiz list
    if args.quiz_id is not None:
        quiz_obj = get_quiz(base_url, token, args.course_id, args.quiz_id,
                            verbose=(args.verbose >= 1))
        quiz_list = [quiz_obj]
    else:
        print(f"Fetching quiz list for course {args.course_id}...")
        quiz_list = get_quizzes(base_url, token, args.course_id,
                                verbose=(args.verbose >= 1))
        quiz_list = [q for q in quiz_list if q.get("published")]
        if not args.include_surveys:
            quiz_list = [q for q in quiz_list
                         if q.get("quiz_type") not in
                         ("survey", "graded_survey")]
        quiz_list.sort(key=lambda q: q.get("title", ""))

    if not quiz_list:
        print("No quizzes matched filters.")
        return

    # Dry-run
    if args.dry_run:
        for q in quiz_list:
            qid = q.get("id", "?")
            title = q.get("title", "(untitled)")
            print(f"  [DRY RUN] quiz {qid}: {title}")
        return

    # Process each quiz
    total = len(quiz_list)
    for i, q in enumerate(quiz_list, 1):
        qid = q.get("id")
        title = q.get("title", "(untitled)")

        # --update: skip if CSV exists
        if args.update and args.outdir:
            slug = sanitize_for_filename(title)
            csv_path = os.path.join(args.outdir, f"{slug}_summary.csv")
            if os.path.isfile(csv_path):
                print(f"[{i}/{total}] Quiz {qid}: {title}  [SKIP]")
                continue

        if total > 1:
            print(f"\n[{i}/{total}] Fetching data for quiz {qid}: {title}")

        title, pts_possible, q_total, qq_lookup, rows = \
            summarize_one_quiz(
                base_url, token, args.course_id, q, verbose=args.verbose)

        print_summary(title, pts_possible, q_total, rows,
                      args.anonymize, verbose=args.verbose)

        # Write CSV if outdir specified
        if args.outdir:
            slug = sanitize_for_filename(title)
            csv_path = os.path.join(args.outdir, f"{slug}_summary.csv")
            write_summary_csv(csv_path, title, pts_possible, q_total,
                              rows, args.anonymize)
            print(f"  [CSV] {csv_path}")


if __name__ == "__main__":
    main()
