#!/usr/bin/env python3
# ============================================================================
# Script:       canvas.api.to_json.py
# Version:      2.0.0
# Date:         2026-02-20
# Purpose:      Fetch Classic Quiz data via Canvas API and write canonical
#               per-submission JSON files.
#
# Usage:        canvas.api.to_json.py COURSE_ID [QUIZ_ID]
#                   [--base-url URL] [--token TOKEN | --token-file FILE]
#                   [--outdir DIR] [--update] [--include-surveys]
#                   [--dry-run] [--verbose] [--help]
#
# Input:        Canvas API token + course_id (or ~/.canvas.api.conf)
# Output:       One canonical JSON file per submission in ./output/JSON
#
# Options:
#   --base-url URL          Canvas instance URL (or set in ~/.canvas.api.conf)
#   --token TOKEN           Inline API token
#   --token-file FILE       Path to token file (or set in ~/.canvas.api.conf)
#   --outdir DIR            Output directory (default: output/JSON)
#   -u, --update            Skip quizzes whose output directory exists
#   --include-surveys       Include survey-type quizzes (batch mode)
#   -n, --dry-run           Show what would be done without making changes
#   -v, --verbose           Show detailed output
#   -h, --help              Show this help message
#
# Session:      Canvas.API
# AI Model:     Claude Opus 4.6
# Attribution:  Generated by Michael Gilchrist in collaboration with
#               ClaudeAI Opus 4.6
# ============================================================================

import argparse
import os
import sys

from canvas_api import (
    get_quiz,
    get_quiz_submission_data,
    get_quiz_submissions,
    get_quizzes,
    get_submission_questions,
)
from canvas_http import http_get_json
from io_utils import load_profile, read_token, sanitize_for_filename, write_json
from model import build_quiz_question_lookup, canonicalize_submission_v2


# ---- Core export logic ----

def _get_quiz_question_metadata(base_url, token, quiz_submission_id, verbose):
    """Fetch quiz question metadata via the Quiz Submission Questions API.

    This endpoint returns both quiz_submission_questions and quiz_questions.
    We use the quiz_questions list which has prompts, answer options, types.
    Falls back to empty list on error.
    """
    url = (f"{base_url}/api/v1/quiz_submissions/{quiz_submission_id}"
           f"/questions?include[]=quiz_question&per_page=100")
    if verbose:
        print(f"  [GET] {url}")
    try:
        data, _headers = http_get_json(url, token)
        return data.get("quiz_questions", [])
    except Exception as e:
        if verbose:
            print(f"  [WARN] Could not fetch quiz question metadata: {e}")
        return []


def export_one_quiz(base_url, token, course_id, quiz,
                    outdir, update, verbose):
    """Export submissions for a single quiz to canonical JSON files.

    Uses the Assignment Submissions API (include[]=submission_history)
    to get actual student answer data (submission_data), and the Quiz
    Submission Questions API for question metadata (prompts, answers).

    Returns (quiz_dir, status_str) where status_str is one of:
    None (success), "skip" (--update), or an error message.
    """
    qid = quiz["id"]
    title = quiz.get("title", "(untitled)")
    slug = sanitize_for_filename(title)
    quiz_dir = os.path.join(outdir, slug)

    # --update: skip if directory already has submission files
    if update and os.path.isdir(quiz_dir):
        existing = [f for f in os.listdir(quiz_dir)
                    if f.startswith("submission_") and f.endswith(".json")]
        if existing:
            return quiz_dir, "skip"

    try:
        assignment_id = quiz.get("assignment_id")
        if not assignment_id:
            return None, f"Quiz {qid} has no linked assignment"

        # Fetch quiz submissions (for timing, score, user_id)
        if verbose:
            print("  [INFO] Fetching quiz submissions...")
        quiz_subs = get_quiz_submissions(
            base_url, token, course_id, qid, verbose=verbose)

        if verbose:
            print(f"  [INFO] {len(quiz_subs)} quiz submissions")

        # Fetch assignment submissions with submission_history
        # (contains submission_data with actual answers)
        if verbose:
            print("  [INFO] Fetching submission answer data...")
        assign_subs = get_quiz_submission_data(
            base_url, token, course_id, assignment_id, verbose=verbose)

        # Build user_id -> submission_data mapping
        user_sub_data = {}
        for asub in assign_subs:
            uid = asub.get("user_id")
            history = asub.get("submission_history", [])
            if history:
                # Use the most recent attempt's submission_data
                latest = history[-1]
                sd = latest.get("submission_data")
                if sd:
                    user_sub_data[uid] = sd

        # Fetch quiz question metadata (prompts, answer options, types)
        # from the first quiz submission we have
        qq_lookup = {}
        if quiz_subs:
            first_sid = quiz_subs[0].get("id")
            if first_sid:
                qq_list = _get_quiz_question_metadata(
                    base_url, token, first_sid, verbose=verbose)
                qq_lookup = build_quiz_question_lookup(qq_list)

        os.makedirs(quiz_dir, exist_ok=True)

        written = 0
        for qs in quiz_subs:
            sid = qs.get("id")
            uid = qs.get("user_id")
            if sid is None:
                continue

            sd = user_sub_data.get(uid, [])
            if not sd and verbose:
                print(f"  [WARN] No submission_data for user {uid} "
                      f"(submission {sid})")

            doc = canonicalize_submission_v2(qs, sd, qq_lookup)
            doc["source"]["mode"] = "api"
            doc["source"]["notes"] = \
                "Canonicalized from Canvas API payloads."
            outpath = os.path.join(quiz_dir, f"submission_{int(sid)}.json")
            write_json(outpath, doc)
            written += 1

        if verbose:
            print(f"  [INFO] Wrote {written} submission files")

        return quiz_dir, None

    except Exception as e:
        return None, str(e)


# ---- CLI ----

def main():
    ap = argparse.ArgumentParser(
        description="Export Classic Quiz submissions to canonical JSON.  "
                    "Pass a QUIZ_ID for a single quiz, or omit it to "
                    "export all published quizzes in the course.")
    ap.add_argument("course_id", type=int,
                    help="Canvas course ID")
    ap.add_argument("quiz_id", nargs="?", type=int, default=None,
                    help="Canvas quiz ID (omit to export all quizzes)")

    ap.add_argument("--base-url", default=None,
                    help="Canvas instance URL (or set base_url in "
                         "~/.canvas.api.conf)")
    ap.add_argument("--token", default=None,
                    help="Canvas API token (inline)")
    ap.add_argument("--token-file", default=None,
                    help="Path to token file (or set token_file in "
                         "~/.canvas.api.conf)")

    ap.add_argument("--outdir", default=os.path.join("output", "JSON"),
                    help="Output directory (default: output/JSON)")

    ap.add_argument("-u", "--update", action="store_true",
                    help="Skip quizzes whose output directory already "
                         "has submission files")
    ap.add_argument("--include-surveys", action="store_true",
                    help="Include survey-type quizzes (batch mode)")

    ap.add_argument("-n", "--dry-run", action="store_true",
                    help="Show what would be done without making changes")
    ap.add_argument("-v", "--verbose", action="store_true",
                    help="Show detailed output")

    args = ap.parse_args()

    profile = load_profile()
    if not args.base_url:
        args.base_url = profile.get("base_url")
    if not args.base_url:
        ap.error("--base-url is required (or set base_url in "
                 "~/.canvas.api.conf)")
    token = read_token(args.token, args.token_file, profile)
    base_url = args.base_url.rstrip("/")

    # Build the list of quizzes to process
    if args.quiz_id is not None:
        # Single-quiz mode
        quiz_obj = get_quiz(base_url, token, args.course_id, args.quiz_id,
                            verbose=args.verbose)
        quiz_list = [quiz_obj]
    else:
        # Batch mode: fetch all published quizzes
        print(f"Fetching quiz list for course {args.course_id}...")
        quiz_list = get_quizzes(base_url, token, args.course_id,
                                verbose=args.verbose)
        quiz_list = [q for q in quiz_list if q.get("published")]
        if not args.include_surveys:
            quiz_list = [q for q in quiz_list
                         if q.get("quiz_type") not in
                         ("survey", "graded_survey")]
        quiz_list.sort(key=lambda q: q.get("title", ""))

    if not quiz_list:
        print("No quizzes matched filters.")
        return

    if len(quiz_list) > 1:
        print(f"Found {len(quiz_list)} quizzes to process.\n")

    # Dry-run output
    if args.dry_run:
        for q in quiz_list:
            qid = q.get("id", "?")
            title = q.get("title", "(untitled)")
            slug = sanitize_for_filename(title)
            qdir = os.path.join(args.outdir, slug)
            exists = os.path.isdir(qdir)
            skip_note = ("  [exists, would skip]"
                         if (args.update and exists) else "")
            print(f"  [DRY RUN] quiz {qid}: {title}{skip_note}")
        return

    # Process each quiz
    ok_count = 0
    skip_count = 0
    fail_count = 0
    total = len(quiz_list)

    for i, q in enumerate(quiz_list, 1):
        qid = q.get("id")
        title = q.get("title", "(untitled)")
        prefix = f"[{i}/{total}] " if total > 1 else ""
        print(f"{prefix}Quiz {qid}: {title}")

        outpath, err = export_one_quiz(
            base_url=base_url,
            token=token,
            course_id=args.course_id,
            quiz=q,
            outdir=args.outdir,
            update=args.update,
            verbose=args.verbose,
        )

        if err == "skip":
            print(f"  [SKIP] Already exists: {outpath}")
            skip_count += 1
        elif err:
            print(f"  [FAIL] {err}", file=sys.stderr)
            fail_count += 1
        else:
            print(f"  [OK] {outpath}")
            ok_count += 1

    # Summary for batch mode
    if total > 1:
        parts = [f"{ok_count} exported"]
        if skip_count:
            parts.append(f"{skip_count} skipped")
        if fail_count:
            parts.append(f"{fail_count} failed")
        print(f"\nDone: {', '.join(parts)} (of {total} quizzes).")


if __name__ == "__main__":
    main()
